
[{"content":"","date":"2024-12-15","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"2024-12-15","externalUrl":null,"permalink":"/tags/emacs/","section":"Tags","summary":"","title":"Emacs","type":"tags"},{"content":"","date":"2024-12-15","externalUrl":null,"permalink":"/categories/emacs/","section":"Categories","summary":"","title":"Emacs","type":"categories"},{"content":"","date":"2024-12-15","externalUrl":null,"permalink":"/","section":"Lizqwerscott 的博客","summary":"","title":"Lizqwerscott 的博客","type":"page"},{"content":" 为啥要关闭 # 当我们进入 meow-beacon-mode 后需要一般都是想要给一些字符加上括号或者变成字符串，而这时候如果默认进入还是自动配对括号，会有问题，想要移动光标，使用 meow 的键位来的话，需要按 esc 那样就直接退出 beacon 模式了。\n实现 # 在 meow-grab 函数启动之前添加 advice, 关闭括号有关的模式\n(advice-add #\u0026#39;meow-grab :before #\u0026#39;(lambda () (call-interactively #\u0026#39;electric-pair-mode) (call-interactively #\u0026#39;awesome-pair-mode))) ","date":"2024-12-15","externalUrl":null,"permalink":"/posts/2024/12/20241214203953-meow_%E8%BF%9B%E5%85%A5_beacon_mode_%E5%90%8E%E5%85%B3%E9%97%AD%E6%8B%AC%E5%8F%B7%E8%87%AA%E5%8A%A8%E9%85%8D%E5%AF%B9%E5%8A%9F%E8%83%BD/","section":"Posts","summary":"","title":"meow 进入 beacon-mode 后关闭括号自动配对功能","type":"posts"},{"content":"","date":"2024-12-15","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"2024-12-15","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" meow 的 beacon-mode # 可以快速的录制 emacs 键盘宏，并且应用到选择的行上边，但最好 meow 进入 beacon-mode 后关闭括号自动配对功能，要不然括号自动配对会对宏编辑产生一些影响。 具体例子可以查看 meow 官方的演示\n多行数字按顺序生成 # 当我们使用 meow 键盘宏生成了多行代码的时候，会遇到想要按照顺序生成数组的下标，比如想要使用 meow 生成如下代码的时候:\nint a[3] = {0, 1, 2}; int c = a[0]; int d = a[1]; int e = a[2]; 只能生成：\nint a[3] = {0, 1, 2}; int c = a[0]; int d = a[0]; int e = a[0]; 这时候就可以使用两种方法进行生成\n矩形区域 # 可以先使用 C-x SPC 选择数字所在的矩形区域 rectangle-mark-mode # 然后使用 C-x r k 删除原本的数字 kill-rectangle # 最后使用 # C-x r N 来生成递增的数字 rectangle-number-lines 最好加上前缀 C-u 可以自定义开始数字和输出格式\n使用正则替换 # 直接使用 replace-regexp 把相同的数字替换成 \\#, 就是按照 0 开头的数字列表了\n使用 visual-replace 需要先打开正则表达式 # visual-replace 官方文档 使用 M-% r 打开或者关闭，需要注意的是如果你使用以下配置配置 visual-replace\n(require \u0026#39;visual-replace) (global-set-key (kbd \u0026#34;s-r\u0026#34;) #\u0026#39;visual-replace) (visual-replace-global-mode 1) 那么 visual-replace 的快捷键都是以你配置的快捷键位开头，就要使用 s-r r 来开启正则功能了\n","date":"2024-12-15","externalUrl":null,"permalink":"/posts/2024/12/20241214205845-meow_%E5%A4%9A%E8%A1%8C%E7%BC%96%E8%BE%91%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E5%B7%A7/","section":"Posts","summary":"","title":"meow 多行编辑的一些技巧","type":"posts"},{"content":"","date":"2024-02-07","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"Ai","type":"tags"},{"content":"","date":"2024-02-07","externalUrl":null,"permalink":"/categories/ai/","section":"Categories","summary":"","title":"Ai","type":"categories"},{"content":" 手动下载模型 # 打开模型的对应页面, 手动点击下载. 缺点: 只能一个一个下载\nGit LFS 模型下载方案（优雅，但不够灵活） # 准备工作 # Git LFS的方案相较于前面自行实现的方案要简洁的多得多. 我们需要在安装 git 的基础上，再安装 git lfs.\ngit lfs install Ubuntu # sudo apt-get install git-lfs 模型下载 # 点击主页下载 # 打开主页面\n点击图中按钮\n点击 Clone respository\n根据图中前两条命令安装\ngit lfs install git clone https://huggingface.co/YeungNLP/firefly-llama2-13b-chat Hugging Face Hub 模型下载方案（优雅，强烈推荐） # 准备工作 # 准备工作同样很简单, 我们只需要安装 huggingface_hub.\npip install huggingface_hub 如果想使用 TUI 管理 huggingface_hub 下载的模型:\npip install -U \u0026#34;huggingface_hub[cli]\u0026#34; 详细内容请查看 官方文档\nArch Linux # 在 Arch Linux 中可以安装 AUR 包: python-huggingface-hub 但是没有 CLI 特性, 无法使用 TUI 界面\n模型下载 # huggingface_hub 提供了很多种模型下载的方案, 详细的可以到 官方文档 进行查看\nsnapshot_download # from huggingface_hub import snapshot_download path = snapshot_download(repo_id=\u0026#34;FlagAlpha/Llama2-Chinese-7b-Chat\u0026#34;) print(path) 忽略一些内容 # 在 snaphot_download 方法中, 提供了 allow_regex 和 ignore_regex 两个参数, 简单来说前者是对指定的匹配项进行下载, 后者是忽略指定的匹配项, 下载其余部分. 我们只需要使用其中一种就可以了.\nsnapshot_download(repo_id=\u0026#34;bert-base-chinese\u0026#34;, ignore_regex=[\u0026#34;*.h5\u0026#34;, \u0026#34;*.ot\u0026#34;, \u0026#34;*.msgpack\u0026#34;]) 使用代理 # 可以使用:\nexport http_proxy=http://127.0.0.1:20171 export https_proxy=http://127.0.01:20171 然后再执行 python 脚本下载.\n","date":"2024-02-07","externalUrl":null,"permalink":"/posts/2024/02/20240207102308-huggingface_%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B/","section":"Posts","summary":"","title":"huggingface 下载模型","type":"posts"},{"content":" 克隆和编译llama.cpp # 下载 llamap.cpp 代码 # git clone https://github.com/ggerganov/llama.cpp （可选）如需使用qX_k 量化方法（相比常规量化方法效果更好） # 请手动打开llama.cpp文件，修改下列行（约2500行左右）\n原始代码 # if (nx % QK_K != 0 || ny % QK_K != 0) { 新代码 # if (nx % QK_K != 0) { 对llama.cpp项目进行编译，生成./main和./quantize二进制文件。 # make Windows/Linux用户 # 推荐与BLAS（或cuBLAS如果有GPU）一起编译，可以提高prompt处理速度，参考: llama.cpp#blas-build\nBLAS 编译命令\nmake LLAMA_OPENBLAS=1 cuBLAS(有GPU) 编译命令\nmake LLAMA_CUBLAS=1 macOS用户 # 无需额外操作，llama.cpp已对ARM NEON做优化，并且已自动启用BLAS。\nM系列芯片 # 推荐使用Metal启用GPU推理，显著提升速度。参考, llama.cpp#metal-build 只需将编译命令改为:\nLLAMA_METAL=1 make 生成量化版本模型 # 本地的 pth 格式模型 # 处理目录结构 # 将合并模型（.pth格式模型）中最后一步生成的 tokenizer.model 文件放入 zh-models 目录下，模型文件 consolidated.*.pth 和配置文件 params.json 放入 zh-models/7B 目录下。请注意 LLaMA 和 Alpaca 的 tokenizer.model 不可混用（原因见训练细节）。例如，如果是.pth格式的模型，目录结构类似：\nllama.cpp/zh-models/\n7B/ consolidated.00.pth params.json tokenizer.model 将上述.pth模型权重转换为 gguf 的FP16格式 # 生成文件路径为 zh-models/7B/ggml-model-f16.bin\npython convert.py zh-models/7B/ 进一步对FP16模型进行4-bit量化 # 生成量化模型文件路径为 zh-models/7B/ggml-model-q4_0.guff。不同量化方法的性能对比见本文最后。\n./quantize ./zh-models/7B/ggml-model-f16.bin ./zh-models/7B/ggml-model-q4_0.bin q4_0 从 huggingface 下载的模型 # 首先下载模型 # 可以从 huggingface 下载模型 了解.\n获取下载目录 # 一般下载好的就在 ~/.cache/huggingface/hub 目录中. 例如, 从 firefly-llama2-13b-chat 下载的就在:\n~/.cache/huggingface/hub/models\u0026ndash;YeungNLP\u0026ndash;firefly-llama2-13b-chat/snapshots/7e72d87fb49a727d9078b4d721e3319f4642f8bc\n转换 # 直接执行这个脚本, 生成的 guff 模型就在你下载的模型文件夹中\npython convert.py /path/model/path 量化 # ./quantize ./path/model-f16.pth ./path/model-q4_0.gguf q4_0 加载并启动模型 # 直接运行 # CPU 直接运行 # ./main -m ./models/chinese-13b/firefly-llama2-13b-chat-q4_0.guff -c 512 -b 8 -n 256 --keep 48 \\ --repeat_penalty 1.0 --color -i -t 16 \\ -r \u0026#34;User:\u0026#34; -f prompts/chat-with-bob.txt CPU 与 GPU 运行 # 如已通过 Metal 编译或者使用了 cuBLAS 编译，则只需加上 -ngl 1 即可启用GPU推理.\n./main -m ./models/chinese-13b/firefly-llama2-13b-chat-q4_0.guff -ngl 1 -c 512 -b 8 -n 256 --keep 48 \\ --repeat_penalty 1.0 --color -i -t 16 \\ -r \u0026#34;User:\u0026#34; -f prompts/chat-with-bob.txt 常用命令 # 在提示符 \u0026gt; 之后输入你的 prompt，cmd/ctrl+c 中断输出，多行信息以 \\ 作为行尾。 如需查看帮助和参数说明，请执行 ./main -h 命令。\n常用运行参数 # -c 控制上下文的长度，值越大越能参考更长的对话历史（默认：512） -ins / -i 启动类 ChatGPT 对话交流的 instruction 运行模式 -f 指定 prompt 模板，alpaca 模型请加载 prompts/alpaca.txt, 所有模板都在 prompts/ 目录下面 -n 控制回复生成的最大长度（默认：128） -b 控制 batch size（默认：8），可适当增加 -t 控制线程数量（默认：4），可适当增加 \u0026ndash;repeat_penalty 控制生成回复中对重复文本的惩罚力度 \u0026ndash;temp 温度系数，值越低回复的随机性越小，反之越大 \u0026ndash;top_p, top_k 控制解码采样的相关参数\n更详细的官方说明请参考：https://github.com/ggerganov/llama.cpp/tree/master/examples/main\n启动服务器 # 和直接运行一样, 可以 CPU 与 GPU 一起运行\n./server -m ./models/chinese-13b/firefly-llama2-13b-chat-q5_k_s.guff -ngl 25 -c 512 -b 8 -n 256 -t 8 更多介绍:\n./server -h 关于量化方法选择及推理速度 # 量化参数介绍: llamap.cpp 量化统计表\n结论 # 结论来源: alpaca llama.cpp 量化部署\n默认的量化方法为 q4_0，虽然速度最快但损失也是最大的，其余方法各有利弊，按实际情况选择 需要注意的是F16以及 q8_0 并不会因为增加线程数而提高太多速度 线程数 -t 与物理核心数一致时速度最快，超过之后速度反而变慢（M1 Max上从8改到10之后耗时变为3倍） 如果使用了Metal版本（即启用了苹果GPU解码），速度还会有进一步显著提升，表中标注为 -ngl 1 综合推荐（仅供参考）：7B推荐 Q5_1 或 Q5_K_S，13B 推荐 Q5_0 或 Q5_K_S 机器资源够用且对速度要求不是那么苛刻的情况下可以使用 q8_0 或 Q6_K，接近 F16 模型的效果 ","date":"2024-02-07","externalUrl":null,"permalink":"/posts/2024/02/20240206214507-llama_cpp_%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2/","section":"Posts","summary":"","title":"llama.cpp 量化部署","type":"posts"},{"content":"","date":"2024-01-09","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"2024-01-09","externalUrl":null,"permalink":"/categories/linux/","section":"Categories","summary":"","title":"Linux","type":"categories"},{"content":" 查看图片 # nsxiv feh ueberzugpp ssh # tssh # 可以搜索, 支持快捷登录 ssh. 文档\ntrzsz # 文档\n安装 # Ubuntu\nsudo apt update \u0026amp;\u0026amp; sudo apt install software-properties-common sudo add-apt-repository ppa:trzsz/ppa \u0026amp;\u0026amp; sudo apt update sudo apt install trzsz Arch\nyay -Syu yay -S trzsz 快速使用指南 # trz 上传文件\ntrz 命令可以不带任何参数，将上传文件到当前目录。也可以带一个目录参数，指定上传到哪个目录。\ntsz 下载文件\ntsz 可以带一个或多个文件名（可使用相对路径或绝对路径，也可使用通配符），将下载指定的文件。\n-q 静默模式\ntrz -q 或 tsz -q xxx ( 加上 -q 选项 )，则在传输文件时不显示进度条。\n-d / -r 传输文件夹\ntrz -d 或 tsz -r xxx ( 加上 -d 或 -r 选项 )，则可以上传或下载指定文件夹和文件。\n-y 覆盖模式\ntrz -y 或 tsz -y xxx ( 加上 -y 选项 )，如果存在相同文件名的文件就直接覆盖，并支持断点续传。\n-b 二进制模式\ntrz -b 或 tsz -b xxx ( 加上 -b 选项 )，二进制传输模式，对于压缩包、图片、影音等较快。\n-e 转义控制字符\n二进制模式时，控制字符可能会导致失败，trz -eb 或 tsz -eb xxx ( 加上 -e 选项 ) 转义所有已知的控制字符。\n-B 缓冲区上限\ntrz -B 20m 或 tsz -B 2M xxx 等，设置最大缓冲区上限 ( 默认 10M )。会自动根据网速选择合适的缓冲区大小，但不会超过此上限。\n-t 超时时间\ntrz -t 30 或 tsz -t 30 xxx 等，设置超时秒数 ( 默认 20 秒 )。在超时时间内，如果无法传完一个缓冲区大小的数据则会报错并退出。设置为 0 或负数，则永不超时。\n有趣 # 黑客帝国 # cmatrix\n统计行数 # tokei # 包管理器 # pacman 使用 # pacman -Ss abc #搜索有关abc信息的包 pacman -Qs abc # 查找本地安装的软件包(模糊字符串查找) pacman -Sy abc #和源同步后安装名为abc的包 pacman -S abc #从本地数据库中得到abc的信息，下载安装abc包 pacman -Sf abc #强制安装包abc pacman -Si abc #从数据库中搜索包abc的信息 pacman -Q # 列出已经安装的软件包 pacman -Q abc # 检查 abc 软件包是否已经安装 pacman -Qi abc #列出已安装的包abc的详细信息 pacman -Ql abc # 列出abc软件包的所有文件 pacman -Qo /path/to/abc # 列出abc文件所属的软件包 pacman -Syu #同步源，并更新系统 pacman -Sy #仅同步源 pacman -Su #更新系统 pacman -R abc #删除abc包 pacman -Rd abc #强制删除被依赖的包 pacman -Rc abc #删除abc包和依赖abc的包 pacman -Rsc abc #删除abc包和abc依赖的包 pacman -Rscn abc #删除abc包和abc依赖的包以及配置文件 pacman -Sc #清理/var/cache/pacman/pkg目录下的旧包 pacman -Scc #清除所有下载的包和数据库 pacman -U abc #安装下载的abs包，或新编译的abc包 pacman -Sd abc #忽略依赖性问题，安装包abc pacman -Su --ignore foo #升级时不升级包foo pacman -Sg abc #查询abc这个包组包含的软件包 SQL # MySQL # mycli # SQLite3 # litecli # pdf # 编辑 # pdfcpu\n查看 # zathura\n文件管理器 # yazi # 预览其他插件\npreview\n预览图片需要 ueberzugpp ranger # joshuto # 压缩 # 解压 # theunarchiver # 支持解压 window 压缩包不乱码, 支持类似 bandizip 的智能解压, 可以自动创建目录.\n注意\n只支持解压\nls # eza # 一个现代的、维护良好的 ls 替代品。\n配置 # 让 ls 支持 git 仓库, 图标, 超链接和颜色\nalias eza=\u0026#34;eza --icons=auto --hyperlink --color=always --color-scale=all --color-scale-mode=gradient --git --git-repos\u0026#34; alias ls=\u0026#34;eza\u0026#34; ","date":"2024-01-09","externalUrl":null,"permalink":"/posts/2024/01/20231127205837-linux_%E8%BD%AF%E4%BB%B6/","section":"Posts","summary":"","title":"Linux 软件","type":"posts"},{"content":" Use Ros cv_bridge with Python3 in ROS Melodic # 背景 # 在使用 ROS Melodic 的时候, cv_bridge默认是使用python2.7. 在 python3 中使用cv bridge 会出现错误.\n解决办法 # 自己编译python3 版本的cv bridge\n依赖 # 使用以下命令安装需要的依赖\nsudo apt-get install python3-pip python-catkin-tools python3-dev python3-numpy sudo pip3 install rospkg catkin_pkg 编译 # 需要创建一个工作空间\nmkdir -p ~/cvbridge_build_ws/src cd ~/cvbridge_build_ws/src 下载noetic版本的cv bridge\n使用这个github仓库的noetic分支, 将代码放到cvbridge_build_ws/src目录下\ngit clone\ngit clone -b noetic https://github.com/ros-perception/vision_opencv.git 修改需要编译的Python版本\n首先需要确保你 Ubuntu 系统的python默认是python3, 修改 vision_opencv/cv_bridge/CMakeLists.txt 将:\nfind_package(Boost REQUIRED python37) 修改为:\nfind_package(Boost REQUIRED python3) 编译\n首先需要知道你的python3路径在那里, 可以使用以下命令获得:\nwhere python3 或者\nwhich python3 然后知道python的include路径, 可以使用以下命令获得:\nimport sys print(sys.executable) #Print python3 executable path print(sys.path) #Print python3 library path 或者使用\npython3-config --includes 然后根据以下获得的变量执行下列命令:\ncd ~/cvbridge_build_ws catkin config -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.6m -DPYTHON_LIBRARY=/usr/lib/aarch64-linux-gnu/libpython3.6m.so catkin config --install 编译cv_bridge\ncatkin build cv_bridge 使用\n当前终端临时使用\nsource install/setup.bash --extend 开机自动使用\n可以添加到.bashrc里面\nsource ~/cvbridge_build_ws/install/setup.bash --extend ","date":"2023-10-25","externalUrl":null,"permalink":"/posts/2023/10/nx%E6%9D%BFcv_bridge%E9%97%AE%E9%A2%98/","section":"Posts","summary":"","title":"nx版cv_bridge问题","type":"posts"},{"content":"","date":"2023-10-25","externalUrl":null,"permalink":"/tags/program/","section":"Tags","summary":"","title":"Program","type":"tags"},{"content":"","date":"2023-10-25","externalUrl":null,"permalink":"/categories/program/","section":"Categories","summary":"","title":"Program","type":"categories"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]